#+Title: CFEngine Enteprise HA Environment

* Bringing up the environment automatically

1) Install the vagrant libvirt plugin if you don't already have it.

   #+BEGIN_SRC shell
     vagrant plugin install vagrant-libvirt
   #+END_SRC

3) Download the CFEngine hub and agent package RPMs for EL6.

   #+BEGIN_SRC shell
     wget "http://buildcache.cloud.cfengine.com/packages/packages-hub-3.15.x/jenkins-bootstrap-packages-3.15.x-11/output/PACKAGES_HUB_x86_64_linux_redhat_6/cfengine-nova-hub-3.15.1-1.el6.x86_64.rpm"
     wget "http://buildcache.cloud.cfengine.com/packages/packages-agent-3.15.x/jenkins-bootstrap-packages-3.15.x-11/output/PACKAGES_x86_64_linux_redhat_6/cfengine-nova-3.15.1-1.el6.x86_64.rpm"
   #+END_SRC
   
4) Set the =HUB_PKG= and =HOSTS_PKG= environment variable.

   #+BEGIN_SRC shell
     export HUB_PKG="cfengine-nova-hub-3.15.1-1.el6.x86_64.rpm"
     export HOSTS_PKG="cfengine-nova-3.15.1-1.el6.x86_64.rpm"
   #+END_SRC
   
5) Initialize the environment.

   #+BEGIN_SRC shell
    ./setup.sh
   #+END_SRC

* Bringing up the environment manually

1) Install the vagrant libvirt plugin if you don't already have it.

   #+BEGIN_SRC shell
     vagrant plugin install libvirt
   #+END_SRC

2) Download the CFEngine hub and agent package RPMs for EL6.

   #+BEGIN_SRC shell
     wget "http://buildcache.cloud.cfengine.com/packages/packages-hub-3.15.x/jenkins-bootstrap-packages-3.15.x-11/output/PACKAGES_HUB_x86_64_linux_redhat_6/cfengine-nova-hub-3.15.1-1.el6.x86_64.rpm"
     wget "http://buildcache.cloud.cfengine.com/packages/packages-agent-3.15.x/jenkins-bootstrap-packages-3.15.x-11/output/PACKAGES_x86_64_linux_redhat_6/cfengine-nova-3.15.1-1.el6.x86_64.rpm"
   #+END_SRC
   
3) Initialize the environment.

   #+BEGIN_SRC shell
     vagrant up node2 && vagrant up node1
   #+END_SRC

   Note: Ordering is important here
   because libvirt will bring up the nodes in parallel and node2 must be up
   before ~pcs cluster auth~.

4) Finish cluster setup manually (if /provisioning/ failed).

   #+BEGIN_EXAMPLE
     ==> node1: Restarting pcsd on the nodes in order to reload the certificates...
     ==> node1: node1: Success
     ==> node1: node2: Success
     ==> node1: ++ pcs cluster start --all
     ==> node1: node1: Unable to connect to node1 (Connection error)
     ==> node1: node2: Unable to connect to node2 (Connection error)
     ==> node1: Error: unable to start all nodes
     ==> node1: node1: Unable to connect to node1 (Connection error)
     ==> node1: node2: Unable to connect to node2 (Connection error)
     The SSH command responded with a non-zero exit status. Vagrant
     assumes that this means the command failed. The output for this command
     should be in the log above. Please read the output to determine what
     went wrong.
   #+END_EXAMPLE

   On Nicks workstation, vagrant errors out after ~pcs cluster start --all~. If
   that happens, *make sure to repeat the failed command and all commands that
   follow the failed one manually*. *All the /provision/ steps are critical.*

* Start PostgreSQL on *node1*

   #+BEGIN_SRC shell
     pushd /tmp; su cfpostgres -c "/var/cfengine/bin/pg_ctl -w -D /var/cfengine/state/pg/data -l /var/log/postgresql.log start"; popd
   #+END_SRC

* Configure PostgreSQL on *node2*

1) Purge the PostgreSQL data directory and initialize the database from the
   state of node1.

   #+BEGIN_SRC shell
     rm -rf /var/cfengine/state/pg/data/*
     pushd /tmp; su cfpostgres -c "cd /tmp && /var/cfengine/bin/pg_basebackup -h node1-pg -U cfpostgres -D /var/cfengine/state/pg/data -X stream -P"; popd
   #+END_SRC

2) Set the recovery mode for PostgreSQL.

   #+BEGIN_SRC shell
     cp /vagrant/recovery.conf /var/cfengine/state/pg/data/recovery.conf
     chown --reference /var/cfengine/state/pg/data/postgresql.conf /var/cfengine/state/pg/data/recovery.conf
   #+END_SRC

* Start and check PostgreSQL then stop it

1) On *node2* start PostgreSQL with the following command.

   #+BEGIN_SRC shell
     pushd /tmp; su cfpostgres -c "/var/cfengine/bin/pg_ctl -D /var/cfengine/state/pg/data -l /var/log/postgresql.log start"; popd
   #+END_SRC

2) On *node2*, check that PostgreSQL is working as a hot standby.

   #+BEGIN_SRC shell
     /var/cfengine/bin/psql -x cfdb -c "SELECT pg_is_in_recovery();"  # should give just 't'
   #+END_SRC

   #+BEGIN_EXAMPLE
     -[ RECORD 1 ]-----+--
     pg_is_in_recovery | t
   #+END_EXAMPLE

3) Check that *node1* is replicating.

   #+BEGIN_SRC shell
     /var/cfengine/bin/psql -x cfdb -c "SELECT * FROM pg_stat_replication;"  # should give state for replication to node2
   #+END_SRC

   Expected output:

   #+BEGIN_EXAMPLE
     -[ RECORD 1 ]----+------------------------------
     pid              | 30731
     usesysid         | 10
     usename          | cfpostgres
     application_name | node2
     client_addr      | 192.168.130.11
     client_hostname  | node2-pg
     client_port      | 60036
     backend_start    | 2018-10-05 19:06:56.599435+00
     backend_xmin     |
     state            | streaming
     sent_lsn         | 0/301F098
     write_lsn        | 0/301F098
     flush_lsn        | 0/301F098
     replay_lsn       | 0/301F098
     write_lag        | 00:00:00.000377
     flush_lag        | 00:00:00.000645
     replay_lag       | 00:00:00.00096
     sync_priority    | 0
     sync_state       | async
   #+END_EXAMPLE

4) Stop PostgreSQL first on *node2*, then on *node1* nodes.

   #+BEGIN_SRC shell
     pushd /tmp; su cfpostgres -c "/var/cfengine/bin/pg_ctl -D /var/cfengine/state/pg/data -l /var/log/postgresql.log stop"; popd
   #+END_SRC

* Configure the PostgreSQL cluster resource

1) Create the base resource.

   #+BEGIN_SRC shell
     pcs resource create cfpgsql pgsql  \
       pgctl="/var/cfengine/bin/pg_ctl" \
       psql="/var/cfengine/bin/psql"    \
       pgdata="/var/cfengine/state/pg/data" \
       pgdb="cfdb" pgdba="cfpostgres" repuser="cfpostgres" \
       tmpdir="/var/cfengine/state/pg/tmp" \
       rep_mode="async" node_list="node1 node2" \
       primary_conninfo_opt="keepalives_idle=60 keepalives_interval=5 keepalives_count=5" \
       master_ip="192.168.130.100" restart_on_promote="true" \
       logfile="/var/log/postgresql.log" \
       config="/var/cfengine/state/pg/data/postgresql.conf" \
       check_wal_receiver=true restore_command="cp /var/cfengine/state/pg/data/pg_arch/%f %p" \
       op monitor timeout="60s" interval="3s" on-fail="restart" role="Master" \
       op monitor timeout="60s" interval="4s" on-fail="restart" --disable
   #+END_SRC

2) Transform/wrap the resource into a Master/Slave resource.

   #+BEGIN_SRC shell
     pcs resource master mscfpgsql cfpgsql master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true
   #+END_SRC

3) Set the constraints for the resource.

   #+BEGIN_SRC shell
     pcs constraint colocation add cfengine with Master mscfpgsql INFINITY
     pcs constraint order promote mscfpgsql then start cfengine symmetrical=false score=INFINITY
     pcs constraint order demote mscfpgsql then stop cfengine symmetrical=false score=0
     pcs constraint location mscfpgsql prefers node1
   #+END_SRC

4) Enable the resource.

   #+BEGIN_SRC shell
     pcs resource enable mscfpgsql --wait=30
   #+END_SRC

5) Check the constraints configuration.

   #+BEGIN_SRC shell
     pcs constraint
   #+END_SRC

   Should give:

   #+BEGIN_SRC
     Location Constraints:
       Resource: mscfpgsql
         Enabled on: node1 (score:INFINITY)
     Ordering Constraints:
       promote mscfpgsql then start cfengine (score:INFINITY) (non-symmetrical)
       demote mscfpgsql then stop cfengine (score:0) (non-symmetrical)
     Colocation Constraints:
       cfengine with mscfpgsql (score:INFINITY) (rsc-role:Started) (with-rsc-role:Master)
     Ticket Constraints:
   #+END_SRC

6) Check the cluster status.

   #+BEGIN_SRC shell
     crm_mon -Afr1
   #+END_SRC

   After a minute or so it should look similar to this:

   #+BEGIN_EXAMPLE
     Stack: cman
     Current DC: node2 (version 1.1.18-3.el6-bfe4e80420) - partition with quorum
     Last updated: Mon Oct  8 19:25:09 2018
     Last change: Mon Oct  8 19:24:59 2018 by root via crm_attribute on node1

     2 nodes configured
     3 resources configured

     Online: [ node1 node2 ]

     Full list of resources:

      Resource Group: cfengine
          cfvirtip	(ocf::heartbeat:IPaddr2):	Started node1
      Master/Slave Set: mscfpgsql [cfpgsql]
          Masters: [ node1 ]
          Slaves: [ node2 ]

     Node Attributes:
     ,* Node node1:
         + cfpgsql-data-status             	: LATEST    
         + cfpgsql-master-baseline         	: 0000000004000098
         + cfpgsql-receiver-status         	: normal (master)
         + cfpgsql-status                  	: PRI       
         + master-cfpgsql                  	: 1000      
     ,* Node node2:
         + cfpgsql-data-status             	: STREAMING|ASYNC
         + cfpgsql-receiver-status         	: normal    
         + cfpgsql-status                  	: HS:async  
         + master-cfpgsql                  	: 100       

     Migration Summary:
     ,* Node node1:
     ,* Node node2:
   #+END_EXAMPLE

   *If the output doesn't look like the example above (one Master, one Slave,
   one =PRI= status, one =HS:async= or =HS:alone= status), try:*

   #+BEGIN_SRC shell
     pcs cluster stop --all && pcs cluster start --all
   #+END_SRC

   and check the status again.

* Check that PostgreSQL HA works

1) Take the *node1* down.

   #+BEGIN_SRC shell
     vagrant halt node1
   #+END_SRC

2) Check that the migration happened and *node2* is now the active (master) node.

   #+BEGIN_SRC shell
     crm_mon -Afr1
   #+END_SRC

   Should give:

   #+BEGIN_SRC
     Stack: cman
     Current DC: node2 (version 1.1.18-3.el6-bfe4e80420) - partition with quorum
     Last updated: Fri Oct  5 10:04:21 2018
     Last change: Fri Oct  5 10:03:48 2018 by root via crm_attribute on node2

     2 nodes configured
     3 resources configured

     Online: [ node2 ]
     OFFLINE: [ node1 ]

     Full list of resources:

      Resource Group: cfengine
          cfvirtip	(ocf::heartbeat:IPaddr2):	Started node2
      Master/Slave Set: mscfpgsql [cfpgsql]
          Masters: [ node2 ]
          Stopped: [ node1 ]

     Node Attributes:
     * Node node2:
         + cfpgsql-data-status             	: LATEST    
         + cfpgsql-master-baseline         	: 0000000005000090
         + cfpgsql-receiver-status         	: ERROR     
         + cfpgsql-status                  	: PRI       
         + master-cfpgsql                  	: 1000      

     Migration Summary:
     * Node node2:
   #+END_SRC

3) Start *node1* again.

   #+BEGIN_SRC shell
     vagrant up node1
   #+END_SRC

4) Check the cluster status.

   #+BEGIN_SRC shell
     crm_mon -Afr1
   #+END_SRC

   Should give something like this (note the /DISCONNECT/ status on *node1*):

   #+BEGIN_SRC shell
     Stack: cman
     Current DC: node2 (version 1.1.18-3.el6-bfe4e80420) - partition with quorum
     Last updated: Fri Oct  5 10:05:51 2018
     Last change: Fri Oct  5 10:03:48 2018 by root via crm_attribute on node2

     2 nodes configured
     3 resources configured

     Online: [ node1 node2 ]

     Full list of resources:

      Resource Group: cfengine
          cfvirtip	(ocf::heartbeat:IPaddr2):	Started node2
      Master/Slave Set: mscfpgsql [cfpgsql]
          Masters: [ node2 ]
          Stopped: [ node1 ]

     Node Attributes:
     * Node node1:
         + cfpgsql-data-status             	: DISCONNECT
         + cfpgsql-status                  	: STOP      
         + master-cfpgsql                  	: -INFINITY 
     * Node node2:
         + cfpgsql-data-status             	: LATEST    
         + cfpgsql-master-baseline         	: 0000000005000090
         + cfpgsql-receiver-status         	: ERROR     
         + cfpgsql-status                  	: PRI       
         + master-cfpgsql                  	: 1000      

     Migration Summary:
     * Node node2:
     * Node node1:
        cfpgsql: migration-threshold=1 fail-count=1000000 last-failure='Fri Oct  5 10:05:33 2018'

     Failed Actions:
     * cfpgsql_start_0 on node1 'unknown error' (1): call=15, status=complete, exitreason='',
         last-rc-change='Fri Oct  5 10:05:33 2018', queued=0ms, exec=121ms
   #+END_SRC

5) Re-initalize node1 with a basebackup from node2.

   #+BEGIN_SRC shell
     rm -rf /var/cfengine/state/pg/data/*
     pushd /tmp; su cfpostgres -c "cd /tmp && /var/cfengine/bin/pg_basebackup -h node2-pg -U cfpostgres -D /var/cfengine/state/pg/data -X stream -P"; popd
   #+END_SRC

6) Set the recovery mode for PostgreSQL.

   #+BEGIN_SRC shell
     cp /vagrant/recovery.conf /var/cfengine/state/pg/data/recovery.conf
     chown --reference /var/cfengine/state/pg/data/postgresql.conf /var/cfengine/state/pg/data/recovery.conf
   #+END_SRC

7) Check that it is the concistency lock causing the failure.

   #+BEGIN_SRC shell
     pcs resource debug-start cfpgsql
   #+END_SRC

   Should give:

   #+BEGIN_SRC
     Operation start for cfpgsql:0 (ocf:heartbeat:pgsql) returned: 'unknown error' (1)
      >  stderr: ERROR: My data may be inconsistent. You have to remove /var/cfengine/state/pg/tmp/PGSQL.lock file to force start.
   #+END_SRC

8) Remove the lock and start the resource.

   #+BEGIN_SRC shell
     rm -f /var/cfengine/state/pg/tmp/PGSQL.lock
     pcs resource debug-start cfpgsql
   #+END_SRC

9) Check the cluster status.

   #+BEGIN_SRC shell
     crm_mon -Afr1
   #+END_SRC

   Should give something like this (i.e. states swapped between node1 and node2 compared to the original state):

   #+BEGIN_SRC shell
     Stack: cman
     Current DC: node2 (version 1.1.18-3.el6-bfe4e80420) - partition with quorum
     Last updated: Fri Oct  5 12:07:38 2018
     Last change: Fri Oct  5 10:09:42 2018 by root via crm_attribute on node2

     2 nodes configured
     3 resources configured

     Online: [ node1 node2 ]

     Full list of resources:

      Resource Group: cfengine
          cfvirtip	(ocf::heartbeat:IPaddr2):	Started node2
      Master/Slave Set: mscfpgsql [cfpgsql]
          Masters: [ node2 ]
          Stopped: [ node1 ]

     Node Attributes:
     * Node node1:
         + cfpgsql-data-status             	: STREAMING|ASYNC
         + cfpgsql-receiver-status         	: normal    
         + cfpgsql-status                  	: HS:async  
         + master-cfpgsql                  	: 100       
     * Node node2:
         + cfpgsql-data-status             	: LATEST    
         + cfpgsql-master-baseline         	: 0000000005000090
         + cfpgsql-receiver-status         	: ERROR     
         + cfpgsql-status                  	: PRI       
         + master-cfpgsql                  	: 1000      

     Migration Summary:
     * Node node2:
     * Node node1:
        cfpgsql: migration-threshold=1 fail-count=1000000 last-failure='Fri Oct  5 10:05:33 2018'

     Failed Actions:
     * cfpgsql_start_0 on node1 'unknown error' (1): call=15, status=complete, exitreason='',
         last-rc-change='Fri Oct  5 10:05:33 2018', queued=0ms, exec=121ms
   #+END_SRC


How to get node1 back from stopped state? Restarting the cluster seems to work, but is there another way?

#+BEGIN_SRC shell
  pcs cluster stop --all && pcs cluster start --all
#+END_SRC

* Configure CFEngine HA

1) Bootstrap *both nodes to node1*.

   #+BEGIN_SRC shell
     cf-agent --bootstrap node1-pg
   #+END_SRC

2) Bootstrap the *node2* to itself.

   #+BEGIN_SRC shell
     cf-agent --bootstrap node2-pg
   #+END_SRC

3) Get the host keys and replace them in the config JSON *on both nodes*.

   #+BEGIN_SRC shell
     cf-key -s
   #+END_SRC

   #+BEGIN_EXAMPLE
     Direction  IP              Name   Last connection           Key
     Incoming   192.168.30.11    node2  Mon Oct  8 16:50:47 2018  SHA=704603762896e830e25fef1ea61e67e9164772026b7919eaa529b7f49003791b
     Outgoing   192.168.30.11    node2  Mon Oct  8 16:50:47 2018  SHA=704603762896e830e25fef1ea61e67e9164772026b7919eaa529b7f49003791b
     Incoming   192.168.100.10  -      Mon Oct  8 17:00:32 2018  SHA=a95e3ac66edbebcc08d2c84f1b2db2e05da4f8d48a98dffc4e9bdd210b698749
     Outgoing   192.168.100.10  -      Mon Oct  8 16:59:43 2018  SHA=a95e3ac66edbebcc08d2c84f1b2db2e05da4f8d48a98dffc4e9bdd210b698749
     Total Entries: 4
   #+END_EXAMPLE


3) Write the HA config JSON *on both nodes* with the correct host key SHAs.


   #+BEGIN_SRC shell
     node1pksha="47df23c4c6eaee944bbc238ce354501bdd6479e5db7e14821ce5d972109af0c7"
     node2pksha="807d6398a5fefbcaf03771250eeff16fcdc3e156230bf4f686c66dd919cb8003"
     cat <<EOF > /var/cfengine/masterfiles/cfe_internal/enterprise/ha/ha_info.json
     {
       "192.168.100.10":
         {
          "sha": "$node1pksha",
          "internal_ip": "192.168.100.10"
         },
       "192.168.100.11":
         {
          "sha": "$node2pksha",
          "internal_ip": "192.168.100.11"
         }
     }
     EOF
   #+END_SRC

4) Enable HA *on both nodes* in the =/var/cfengine/masterfiles/controls/def.cf=
   file by uncommenting the following line and commenting out the line above it.

   #+BEGIN_SRC
     #"enable_cfengine_enterprise_hub_ha" expression => "enterprise_edition";
   #+END_SRC

5) Trigger the policy update *on both nodes*.

   #+BEGIN_SRC shell
     cf-agent -Kf update.cf
   #+END_SRC

6) Restart CFEngine (may not be needed?). I think this is necessary if MP is to
   be run on the VIP. Also in order for MP to show status reports must be
   collected from both node1 and node2. Until then it may show that HA is not
   configured or that it is degraded because one of the hosts is not reporting
   with HA status.

   #+BEGIN_SRC shell
     service cfengine3 restart
   #+END_SRC
